{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"mount_file_id":"15wVNjaXYS2LTEDTrszwVO4XJiSMobzpS","authorship_tag":"ABX9TyPvLxzF9h2g5XK/Rdkykni+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"T6--m3zZ40Dy","executionInfo":{"status":"ok","timestamp":1602582175983,"user_tz":-180,"elapsed":245579,"user":{"displayName":"Carlos Salas","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjHcSQ5_YYk7-ZThuDlx76VCkP-s7Q-BCd_xC2N=s64","userId":"17814195931647351993"}}},"source":["import zipfile\n","\n","with zipfile.ZipFile(\"drive/My Drive/ANN/SR/Spectrogram.zip\",\"r\") as z:\n","    z.extractall(\".\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rp1xJkQWrhJh","outputId":"f9707a17-cb5c-4822-fd70-48098dafbfed","colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#!pip install keras\n","#!pip install tensorflow\n","#!pip install opencv-python\n","from __future__ import print_function\n","\n","from keras.models import Sequential\n","from keras.layers import Convolution2D, MaxPooling2D, MaxPool2D\n","from keras.layers import Activation, Dropout, Flatten, Dense, BatchNormalization\n","from keras.preprocessing.image import ImageDataGenerator\n","\n","from keras.constraints import maxnorm\n","from keras.layers.convolutional import Conv2D\n","\n","\n","from keras import callbacks\n","from keras import optimizers\n","\n","import os\n","\n","import keras.models\n","\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n","\n","import tensorflow as tf\n","import cv2\n","import sys\n","\n","from keras import backend as K\n","\n","from keras.utils import np_utils\n","from IPython.display import clear_output\n","\n","def clear_output():  \n","    clear_output()\n","\n","def cnn_TrainTest(no_of_epochs, no_of_gpus, train_b_size, valid_b_size, data_type,  experiment_folder, input_data_dir, data_sub_type='',\n","                  setSize = 1, width=128, height=128, chanels = 3, test_b_size=32):\n","\n","    sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\n","    tf.compat.v1.keras.backend.set_session(sess)\n","\n","    \n","    epochs = no_of_epochs #50-100-150\n","    gpu_no = no_of_gpus #1-2\n","    train_batch_size = train_b_size #32\n","    valid_batch_size = valid_b_size #32\n","    test_batch_size = test_b_size #32\n","\n","    setDimensions = setSize #1\n","    img_height = width #128\n","    img_width = height #128\n","    channels = chanels #3\n","\n","    class_names = []\n","\n","    datatype = data_type #'Cepstr'\n","    exp_folder = experiment_folder #'no_of_classes_111'\n","\n","    if data_sub_type =='':\n","        #train_data_dir = input_data_dir + datatype + '/' + exp_folder + '/train/'\n","        train_data_dir = input_data_dir + '/train/'\n","        validation_data_dir = input_data_dir + '/validate/'\n","        test_data_dir = input_data_dir + '/test/'\n","        #train_data_dir = input_data_dir\n","        #validation_data_dir = input_data_dir + datatype + '/' + exp_folder + '/validate/'\n","        #test_data_dir = input_data_dir + datatype + '/' + exp_folder + '/test/'\n","        path = input_data_dir + 'Results'\n","    else:\n","        train_data_dir = input_data_dir + datatype + '/' + data_sub_type +'/'+ exp_folder + '/train/'\n","        validation_data_dir = input_data_dir + datatype + '/' + data_sub_type +'/'+ exp_folder + '/validate/'\n","        test_data_dir = input_data_dir + datatype + '/' + data_sub_type +'/'+ exp_folder + '/test/'\n","        path = input_data_dir +'Results/' + data_sub_type\n","\n","\n","    run_name = '_' + datatype + '_' + exp_folder + '_' + str(train_batch_size) + '_' + str(valid_batch_size)\n","\n","    graphPath = 'drive/My Drive/ANN/SR/' + 'Graph/' + run_name +'/'\n","    csvPath = 'drive/My Drive/ANN/SR/'\n","    checkpointerPath = 'drive/My Drive/ANN/SR/' + 'Model/'\n","    predictionsPath = 'drive/My Drive/ANN/SR/'\n","\n","    if not os.path.exists(graphPath):\n","        os.makedirs(graphPath)\n","    if not os.path.exists(csvPath):\n","        os.makedirs(csvPath)\n","    if not os.path.exists(checkpointerPath):\n","        os.makedirs(checkpointerPath)\n","    if not os.path.exists(predictionsPath):\n","        os.makedirs(predictionsPath)\n","\n","    \n","    \n","    csvPath = 'drive/My Drive/ANN/SR/' + run_name + '_loss.csv'\n","    checkpointerPath = 'drive/My Drive/ANN/SR/' + run_name + '.h5'\n","    summaryPath = 'drive/My Drive/ANN/SR/' + run_name + '_summary.csv'\n","    predictionsPath = 'drive/My Drive/ANN/SR/' + run_name + '_predictions.csv'\n","\n","    class_names = [d for d in os.listdir(train_data_dir)]\n","    no_of_classes = len(class_names)\n","\n","    train_file_no = 0\n","    aa = 1\n","    for x in class_names:\n","        #print(x)\n","        list_dir = os.path.join(train_data_dir, x)\n","        for name in os.listdir(list_dir):\n","            isfile = os.path.isfile(list_dir + '/' + name)\n","            if isfile:\n","                train_file_no = train_file_no + 1;  # count files\n","            if aa == 1 and isfile:  # for one time set the tensor shape\n","                img = cv2.imread(os.path.join(list_dir + '/', name))\n","                if setDimensions == 0:  # if do not set dimmensions e3xplicitly do it from the first file\n","                    img_height, img_width, channels = img.shape\n","                # set the tensor shape according to image size\n","                if K.image_data_format() == 'channels_first':\n","                    input_shape = (channels, img_width, img_height)\n","                else:\n","                    input_shape = (img_width, img_height, channels)  # tensorflow\n","                aa = 2\n","\n","    validation_file_no = 0\n","    for x in class_names:\n","        list_dir = os.path.join(validation_data_dir, x)\n","        for name in os.listdir(list_dir):\n","            isfile = os.path.isfile(list_dir + '/' + name)\n","            if isfile:\n","                validation_file_no = validation_file_no + 1;  # count files\n","\n","    test_file_no = 0\n","    for x in class_names:\n","        list_dir = os.path.join(test_data_dir, x)\n","        for name in os.listdir(list_dir):\n","            isfile = os.path.isfile(list_dir + '/' + name)\n","            if isfile:\n","                test_file_no = test_file_no + 1;  # count files\n","\n","    train_batches = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n","        train_data_dir,\n","        target_size=(img_width, img_height),\n","        classes=class_names,\n","        shuffle=True,\n","        class_mode='categorical',\n","        batch_size=train_batch_size)\n","\n","    valid_batches = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n","        validation_data_dir,\n","        target_size=(img_width, img_height),\n","        classes=class_names,\n","        shuffle=True,\n","        batch_size=valid_batch_size,\n","        class_mode='categorical')\n","\n","    test_batches = ImageDataGenerator(rescale=1. / 255).flow_from_directory(\n","        test_data_dir,\n","        target_size=(img_width, img_height),\n","        classes=class_names,\n","        shuffle=False,\n","        batch_size=test_batch_size,\n","        class_mode='categorical')\n","\n","    # TEST show images\n","    # imgs,labels = next(train_batches)\n","    # showImages.plots(imgs, titles=labels)\n","\n","\n","\n","    # network topology\n","\n","    # model = keras.models.load_model('drive/My Drive/ANN/SR/')\n","    #if (os.path.exists('drive/My Drive/ANN/SR/convmore_best_model.hdf5')):\n","    #    print('Loading Previous model...')\n","    #    model = keras.models.load_model(\"drive/My Drive/ANN/SR/convmore_best_model.hdf5\")\n","    #    model.load_weights(\"drive/My Drive/ANN/SR/convmore_best_model.hdf5\")\n","    #    print(model.summary())\n","        \n","    #else:\n","      #model = Sequential()\n","\n","      #model.add(Convolution2D(32, (3, 3), input_shape=input_shape))\n","      #model.add(Activation('relu'))\n","      #model.add(BatchNormalization())\n","      #model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","      #model.add(Convolution2D(64, (2, 2)))\n","      #model.add(Activation('relu'))\n","      #model.add(BatchNormalization())\n","      #model.add(MaxPooling2D(pool_size=(3, 3)))\n","\n","      #model.add(Convolution2D(64, (2, 2)))\n","      #model.add(Activation('relu'))\n","      #model.add(BatchNormalization())\n","      #model.add(MaxPooling2D(pool_size=(2, 2)))\n","\n","      #model.add(Flatten())\n","\n","      #model.add(Dense(64))\n","      #model.add(Activation('relu'))\n","      #model.add(BatchNormalization())\n","\n","      #model.add(Dense(no_of_classes))\n","      #model.add(Activation('softmax'))\n","\n","    model = Sequential()\n","    model.add(tf.keras.layers.Reshape((128,128,3), input_shape=input_shape))\n","    model.add(tf.keras.layers.Conv2D(filters=10, kernel_size=(3,3),data_format=\"channels_last\"))\n","    model.add(tf.keras.layers.LayerNormalization(axis=1 , center=True , scale=True))\n","    \n","    model.add(Conv2D(64, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(64, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(MaxPooling2D(pool_size=(2, 2)))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(128, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(128, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Conv2D(128, (3, 3), padding='same'))\n","    model.add(Activation('relu'))\n","    model.add(Dropout(0.2))\n","\n","    model.add(Flatten())\n","    model.add(Dropout(0.2))\n","\n","    model.add(Dense(no_of_classes))\n","    model.add(Activation('softmax'))\n","\n","    # opt = SGD(lr=2e-3, momentum=0.9)\n","\n","\n","    batches_per_epoch = train_file_no/train_b_size\n","    lr_decay = (1./0.75 -1)/batches_per_epoch\n","\n","    opt = optimizers.Adam(lr=1e-5,decay=lr_decay, beta_1=0.95, beta_2=0.999, epsilon=1e-08)\n","\n","    print(model.summary())\n","\n","    #if gpu_no > 1:\n","    #    model = multi_gpu.make_parallel(model, gpu_no)\n","\n","    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n","    # model.save('drive/My Drive/ANN/SR/')\n","    csv_log = callbacks.CSVLogger('drive/My Drive/ANN/SR/accuracy.csv', separator=',', append=True)\n","\n","    checkpointer = callbacks.ModelCheckpoint(filepath=\"drive/My Drive/ANN/SR/best_model.hdf5\",\n","                                             verbose=0, monitor='val_accuracy', \n","                                             save_best_only=True, mode='max',\n","                                             save_weights_only=True)\n","    #checkpt = callbacks.ModelCheckpoint(filepath=\"drive/My Drive/ANN/SR/best_model.hdf5\", monitor='val_accuracy', verbose=0,\n","    #save_weights_only=False, save_best_only=True, mode='max', period=10)\n","    checkpt = callbacks.ModelCheckpoint(filepath=\"drive/My Drive/ANN/SR/lessdropout_2dense_1conv.hdf5\", monitor='val_accuracy', verbose=1,\n","                                        save_weights_only=False, save_best_only=True, mode='max', period=1)\n","    # early_stopping=callbacks.EarlyStopping(monitor='val_loss', min_delta= 0, patience= 0, verbose= 0, mode= 'min')\n","\n","    tbCallBack = keras.callbacks.TensorBoard(log_dir='drive/My Drive/ANN/SR/logs', histogram_freq=0, write_graph=True, write_images=True)\n","    \n","    # print(len(test_img_data))\n","    history = model.fit_generator(\n","        train_batches,\n","        steps_per_epoch=(train_file_no // train_batch_size + 1) * gpu_no,\n","        epochs=epochs,\n","        verbose=1,\n","        validation_data=valid_batches,\n","        validation_steps=(validation_file_no // valid_batch_size + 1) * gpu_no,\n","        callbacks=[checkpt, tbCallBack, csv_log]\n","        #callbacks=[tbCallBack, csv_log]\n","    )\n","    print(history.history.keys())\n","    print('\\nTesting model')\n","\n","    # model = load_model(path + 'Models/02ClNoNoise/' + name + '.h5')\n","\n","    predictionResult = model.predict_generator(test_batches, steps=test_file_no // test_batch_size + 1, verbose=0)\n","    f_names = test_batches.filenames\n","    # test_imgs, test_labels = next(test_batches)\n","\n","    tmp = test_batches.class_indices\n","    tmp_batch_files = test_batches.filenames\n","    Y_true = []\n","    for t in tmp_batch_files:\n","        for item in tmp:\n","            if t.split(\"/\")[0] == item:\n","                Y_true.append(tmp[item])\n","\n","    Y_true = np_utils.to_categorical(Y_true, no_of_classes)\n","\n","    # original = sys.stdout\n","    file_w = open(predictionsPath, 'w')\n","    file_s = open(summaryPath, 'w')\n","\n","    predictions = np.argmax(predictionResult, axis=1)\n","    file_w.write(run_name)\n","    file_s.write(run_name)\n","\t\n","    #print(run_name, file_w)\n","    #print(run_name, file_s)\n","    #print>> file_w, run_name\n","    #print>> file_s, run_name\n","\n","\n","    #print(\"\\n\")\n","    #print(\"acc: \" + str(history.history[\"acc\"][epochs - 1]) + \" loss: \" + str(\n","    #    history.history[\"loss\"][epochs - 1]) +\n","    #      \" val_acc: \" + str(history.history[\"val_acc\"][epochs - 1]) + \" val_loss: \" + str(\n","    #    history.history[\"val_loss\"][epochs - 1]))\n","\n","    aaa = \"acc: \" + str(history.history[\"accuracy\"][epochs - 1]) + \" loss: \" + str(\n","        history.history[\"loss\"][epochs - 1]) + \" val_acc: \" + str(\n","        history.history[\"val_accuracy\"][epochs - 1]) + \" val_loss: \" + str(history.history[\"val_loss\"][epochs - 1])\n","\n","    bbb = \"train batch size - \" + str(train_batch_size) + \" validation batch size - \" + str(valid_batch_size)\n","\n","    file_s.write(\"\\n\")\n","    file_s.write(\"Stats\")\n","    file_s.write(aaa)\n","    #print(\"\\n\", file_s)\n","    #print(\"Stats\", file_s)\n","    #print(aaa, file_s)\n","    #print>> file_s, \"\\n\"\n","    #print>> file_s, \"Stats\"\n","    #print>> file_s, aaa\n","\n","    file_s.write(\"\\n\")\n","    file_s.write(\"Batches\")\n","    file_s.write(aaa)\n","    #print(\"\\n\", file_s)\n","    #print(\"Batches\", file_s)\n","    #print(bbb, file_s)\n","    #print>> file_s, \"\\n\"\n","    #print>> file_s, \"Batches\"\n","    #print>> file_s, bbb\n","\n","    file_s.write(\"\\n\")\n","    file_s.write(\"Clasification Report\")\n","    #print(\"\\n\", file_s)\n","    #print(\"Clasification Report\", file_s)\n","    #print>> file_s, \"\\n\"\n","    #print>> file_s, \"Classification report\"\n","    cr = classification_report(y_true=np.argmax(Y_true, axis=1), y_pred=predictions,\n","                               target_names=class_names)\n","    file_s.write(cr)    \n","    print(cr, file_s)\n","    # print>> file_s, cr\n","    # print(classification_report(y_true=np.argmax(Y_true, axis=1), y_pred=predictions,\n","    #                            target_names=class_names))\n","\n","    file_s.write(\"\\n\")\n","    file_s.write(\"Confusion Matrix\")\n","    #print(\"\\n\", file_s)\n","    #print(\"Confusion Matrix\", file_s)\n","    \n","    #print>> file_s, \"\\n\"\n","    #print>> file_s, \"Confusion Matrix\"\n","    cm = confusion_matrix(np.argmax(Y_true, axis=1), predictions)\n","    qq = cm.tolist()\n","    for item in qq:\n","        # print(item, file_s)\n","        file_s.writelines(str(item))\n","        # print>> file_s, item\n","    #print>> file_s, qq\n","    # print(confusion_matrix(np.argmax(Y_true, axis=1), predictions))\n","    file_s.close()\n","\n","    file_w.write(\"\\n\")\n","    file_w.write(\"Prediction Result\")\n","    # print(\"\\n\", file_w)\n","    # print(\"Prediction Result\", file_w)\n","    # print>> file_w, \"\\n\"\n","    # print>> file_w, \"Prediction Result\"\n","    qq = predictionResult.tolist()\n","    for item in qq:\n","        # print>> file_w, item\n","        file_w.writelines(str(item))\n","        # print(item, file_w)\n","\n","    file_w.write(\"\\n\")\n","    file_w.write(\"Classification Result\")\n","    # print(\"\\n\", file_w)\n","    # print(\"Classification Result\", file_w)\n","    # print>> file_w, \"\\n\"\n","    # print>> file_w, \"Classification Result\"\n","    qq = predictionResult.tolist()\n","    for item in qq:\n","        maxVal = max(item)\n","        maxValIndx = item.index(max(item))\n","        newRow = [0] * len(item)\n","        newRow[maxValIndx] = 1\n","        # print(newRow, file_w)\n","        file_w.writelines(str(newRow))\n","        # file_w.write(\"Prediction Result\")\n","        #print>> file_w, newRow\n","\n","    # print(\"\\n\")\n","    # print(\"Predictions\")\n","    # print(predictionResult)\n","    file_w.write(\"\\n\")\n","    file_w.write(\"True Classes\")\n","    # print(\"\\n\", file_w)\n","    # print(\"True classes\", file_w)\n","    # print>> file_w, \"\\n\"\n","    # print>> file_w, \"True classes\"\n","    qq = Y_true.tolist()\n","    for item in qq:\n","        #print>> file_w, item\n","        file_w.writelines(str(item))\n","        # print(item, file_w)\n","    # print>>file_w, Y_true\n","\n","    # print(\"\\n\")\n","    # print(\"True classes\")\n","    # print(Y_true)\n","    file_w.write(\"\\n\")\n","    file_w.write(\"Filenames\")\n","    # print(\"\\n\", file_w)\n","    # print(\"Filenames\", file_w)\n","    # print>> file_w, \"\\n\"\n","    # print>> file_w, \"Filenames\"\n","    for item in f_names:\n","        # print>> file_w, item\n","        file_w.write(item)\n","        # print(item, file_w)\n","    # print>>file_w, f_names\n","\n","    # print(\"\\n\")\n","    # print(\"Filenames\")\n","    # print(f_names)\n","\n","    file_w.close()\n","\n","    # sys.stdout = original\n","    sess.close()\n","\n","    print('done')\n","\n","cnn_TrainTest(10,1,64,32,'Spectrogram_full','no_of_classes_10','Spectrogram/' )\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Device mapping:\n","/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n","/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:04.0, compute capability: 7.0\n","\n","Found 497505 images belonging to 111 classes.\n","Found 62085 images belonging to 111 classes.\n","Found 62085 images belonging to 111 classes.\n","Model: \"sequential_4\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","reshape_4 (Reshape)          (None, 128, 128, 3)       0         \n","_________________________________________________________________\n","conv2d_27 (Conv2D)           (None, 126, 126, 10)      280       \n","_________________________________________________________________\n","layer_normalization_4 (Layer (None, 126, 126, 10)      252       \n","_________________________________________________________________\n","conv2d_28 (Conv2D)           (None, 126, 126, 64)      5824      \n","_________________________________________________________________\n","activation_39 (Activation)   (None, 126, 126, 64)      0         \n","_________________________________________________________________\n","max_pooling2d_8 (MaxPooling2 (None, 63, 63, 64)        0         \n","_________________________________________________________________\n","dropout_27 (Dropout)         (None, 63, 63, 64)        0         \n","_________________________________________________________________\n","conv2d_29 (Conv2D)           (None, 63, 63, 64)        36928     \n","_________________________________________________________________\n","activation_40 (Activation)   (None, 63, 63, 64)        0         \n","_________________________________________________________________\n","max_pooling2d_9 (MaxPooling2 (None, 31, 31, 64)        0         \n","_________________________________________________________________\n","dropout_28 (Dropout)         (None, 31, 31, 64)        0         \n","_________________________________________________________________\n","conv2d_30 (Conv2D)           (None, 31, 31, 128)       73856     \n","_________________________________________________________________\n","activation_41 (Activation)   (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","dropout_29 (Dropout)         (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","conv2d_31 (Conv2D)           (None, 31, 31, 128)       147584    \n","_________________________________________________________________\n","activation_42 (Activation)   (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","dropout_30 (Dropout)         (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","conv2d_32 (Conv2D)           (None, 31, 31, 128)       147584    \n","_________________________________________________________________\n","activation_43 (Activation)   (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","dropout_31 (Dropout)         (None, 31, 31, 128)       0         \n","_________________________________________________________________\n","flatten_4 (Flatten)          (None, 123008)            0         \n","_________________________________________________________________\n","dropout_32 (Dropout)         (None, 123008)            0         \n","_________________________________________________________________\n","dense_16 (Dense)             (None, 111)               13653999  \n","_________________________________________________________________\n","activation_44 (Activation)   (None, 111)               0         \n","=================================================================\n","Total params: 14,066,307\n","Trainable params: 14,066,307\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n","Epoch 1/10\n","3876/7774 [=============>................] - ETA: 7:53 - loss: 1.5811 - accuracy: 0.6364"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"85n95XDwwzCa"},"source":["Sep-28 - Mix up the training/validation data. Maybe using only noise data for train messes up non-noise validation."]},{"cell_type":"markdown","metadata":{"id":"-O2m6wYI0Nlv"},"source":["Sep-30 - Changing data helped to get good validation accuracy results, but validation loss is stuck at ~.44, so maybe simplifing the model could help."]},{"cell_type":"markdown","metadata":{"id":"g91ZuR7IXzeW"},"source":["Oct-1 - Simplifing the model gave worst results, so maybe making it more complex should work? Using a VGG-16 based model (But with more complex structure) might be a good idea."]},{"cell_type":"markdown","metadata":{"id":"e-0b-tWCYJcP"},"source":["Oct-6 - VGG-16 model by itself gave a deadlock at .009 of val_acc, so probably the best approach is going back to the manually optimized model and keep tunning it."]}]}